
<h4>Benjamin (Ben) Tremblay</h4>


Hi, I'm Ben—an undergraduate CS student at <strong>William &amp; Mary</strong> specializing in AI/ML. 
Under the mentorship of <a href="https://antoniomastropaolo.github.io">Dr. Antonio Mastropaolo</a>, I research the intersection of software engineering and AI optimization, with particular focus on <em>large code models</em>. 
My work explores <em>parameter-efficient fine-tuning (PEFT)</em> and <em>neural compression techniques</em>—including quantization, pruning, and adapter methods like LoRA/QLoRA—applied to code generation models. 
The central question driving my research is: <strong>when optimization methods alter a model's underlying topology, what happens to code quality beyond mere correctness?</strong> 
While functional correctness might be preserved, I investigate whether other critical software attributes—maintainability, security, readability, and adherence to best practices—degrade under these structural transformations. 
My goal is to develop evaluation frameworks and optimization strategies that achieve computational efficiency without sacrificing the holistic quality of generated code.



<div style="display: grid; grid-template-columns: repeat(4, 1fr); gap: 15px; padding: 5px;">
    <a href="https://www.google.com"><i class="fa-solid fa-globe fa-2x"></i></a>
    <a href="https://www.linkedin.com/in/benjamin-tremblay-29a510247/"><i class="fa-brands fa-linkedin fa-2x"></i></a>
    <a href="https://x.com/"><i class="fa-brands fa-x-twitter fa-2x"></i></a>
    <a href="https://scholar.google.com/"><i class="fa-brands fa-google-scholar fa-2x"></i></a>
</div>


