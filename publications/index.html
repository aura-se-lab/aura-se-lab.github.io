<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | The AURA Lab </title> <meta name="author" content="The AURA Lab"> <meta name="description" content="Founded in 2024, AURA Lab advances AI for software engineering with a focus on transparency and resource‑efficient automation. Through disciplined engineering and real‑world validation, the lab turns research into dependable tools that improve everyday developer work. Led by Dr. Antonio Mastropaolo at William &amp; Mary, the lab develops approaches in: Explainability &amp; Interpretability — methods that reveal how code models reason (rationales, traces, tests) so developers can inspect, trust, and debug automation. Efficiency &amp; Resource‑Aware Automation — quantization, distillation, and PEFT to deliver low‑latency, memory‑light, cost‑aware automation measured on real workloads with reproducible metrics. Multi‑Agent LLMs for SE — role‑aligned agents that plan, code, test, and review, coordinated to produce coherent end‑to‑end developer assistance. Neurosymbolic AI for SE — hybrid pipelines pairing LLMs with program analysis and constraints to produce interpretable, checkable fixes, tests, and designs. Task‑Aware Code Automation — from issues, diffs, and traces to concise summaries, TODOs, and plans; models that reduce developer toil with actionable, repo‑aware guidance. "> <meta name="keywords" content="Research Group, AURA Lab, AI and Software Engineering, Sustainability, Interpretability"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/research.png?dd090c709718990e58c01621e29eb303"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://aura.se.lab.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">The</span> AURA Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Ongoing Research </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">People </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="vitale2025optimizing" class="col-sm-8"> <div class="title">Optimizing Datasets for Code Summarization: Is Code-Comment Coherence Enough?</div> <div class="author"> Antonio Vitale, Antonio Mastropaolo, Rocco Oliveto, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Massimiliano Di Penta, Simone Scalabrino' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Proceedings of the 33rd IEEE/ACM International Conference on Program Comprehension (ICPC 2025)</em>, Jan 2025 </div> <div class="periodical"> To Appear – ICPC 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2502.07611" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Automated code summarization is a long-standing goal for code comprehension. This task automatically generates documentation using a given method. Deep Learning (DL)-based approaches have been proven beneficial for various software engineering (SE) tasks, including this one. Most state-of-the-art datasets for code summarization are automatically mined from GitHub and, thus, might contain erroneous or sub-optimal examples. Previous work showed that using a simple rule-based approach for removing noisy instances allows for a tangible reduction of the training set size while not reducing the effectiveness of the trained models. Motivated by this finding, we conjecture that it is possible to further reduce the dataset size by removing instances that contain different issues. In this paper, we explore the extent to which code-comment coherence, a specific quality attribute of code summaries, can be used to optimize code summarization datasets. Specifically, we hypothesize that removing incoherent code-comment pairs might positively impact the effectiveness of the models. To do this, we rely on SIDE, a recently introduced metric for code-summary coherence. We examine multiple selectivity levels of training instances from two state-of-the-art datasets (TL-CodeSum and Funcom) and evaluate the resulting models on three manually curated test sets. The results show that even halving the training set sizes does not significantly affect the model’s ability to generate summaries. However, when comparing the most restrictive selection strategy with a simpler one that randomly selects the training instances, we observe that the resulting accuracy of the model also does not change. This result suggests that (i) current datasets contain many irrelevant examples, and (ii) different quality attributes should be explored for optimizing code summarization datasets.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">vitale2025optimizing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Optimizing Datasets for Code Summarization: Is Code-Comment Coherence Enough?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vitale, Antonio and Mastropaolo, Antonio and Oliveto, Rocco and Di Penta, Massimiliano and Scalabrino, Simone}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the 33rd IEEE/ACM International Conference on Program Comprehension (ICPC 2025)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{To Appear -- ICPC 2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Software engineering, Artificial Intelligence, Code Summarization, Optimization, Datasets, LLMs}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="afrin2025resource" class="col-sm-8"> <div class="title">Resource-Efficient &amp; Effective Code Summarization</div> <div class="author"> Saima Afrin, Joseph Call, Khai-Nguyen Nguyen, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Oscar Chaparro, Antonio Mastropaolo' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Proceedings of the 2nd ACM international conference on AI Foundation Models and Software Engineering (FORGE 2025)</em>, Jan 2025 </div> <div class="periodical"> To Appear </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2502.03617" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Code Language Models (CLMs) have demonstrated high effectiveness in automating software engineering tasks such as bug fixing, code generation, and code documentation. This progress has been driven by the scaling of large models, ranging from millions to trillions of parameters (e.g., GPT-4). However, as models grow in scale, sustainability concerns emerge, as they are extremely resource-intensive, highlighting the need for efficient, environmentally conscious solutions. GreenAI techniques, such as QLoRA (Quantized Low-Rank Adaptation), offer a promising path for dealing with large models’ sustainability as they enable resource-efficient model fine-tuning. Previous research has shown the effectiveness of QLoRA in code-related tasks, particularly those involving natural language inputs and code as the target output (NL-to-Code), such as code generation. However, no studies have explored its application to tasks that are fundamentally similar to NL-to-Code (natural language to code) but operate in the opposite direction, such as code summarization. This leaves a gap in understanding how well QLoRA can generalize to Code-to-NL tasks, which are equally important for supporting developers in understanding and maintaining code. To address this gap, we investigate the extent to which QLoRA’s capabilities in NL-to-Code tasks can be leveraged and transferred to code summarization, one representative Code-to-NL task. Our study evaluates two state-of-the-art CLMs (CodeLlama and DeepSeek-Coder) across two programming languages: Python and Java. Our research tasked models with generating descriptions for Python and Java code methods. The results align with prior findings on QLoRA for source code generation, showing that QLoRA enables efficient fine-tuning of CLMs for code summarization.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">afrin2025resource</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Resource-Efficient \&amp; Effective Code Summarization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Afrin, Saima and Call, Joseph and Nguyen, Khai-Nguyen and Chaparro, Oscar and Mastropaolo, Antonio}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the 2nd ACM international conference on AI Foundation Models and Software Engineering (FORGE 2025)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{To Appear}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Software engineering, Artificial Intelligence, Code Summarization, Optimization, Datasets, LLMs}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="velasco2025toward" class="col-sm-8"> <div class="title">Toward Neurosymbolic Program Comprehension</div> <div class="author"> Alejandro Velasco, Aya Garryyeva, David N Palacio, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Antonio Mastropaolo, Denys Poshyvanyk' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Proceedings of the 33rd IEEE/ACM International Conference on Program Comprehension (ICPC-ERA 2025) </em>, Jan 2025 </div> <div class="periodical"> To Appear </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2502.01806" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Recent advancements in Large Language Models (LLMs) have paved the way for Large Code Models (LCMs), enabling automation in complex software engineering tasks, such as code generation, software testing, and program comprehension, among others. Tools like GitHub Copilot and ChatGPT have shown substantial benefits in supporting developers across various practices. However, the ambition to scale these models to trillion-parameter sizes, exemplified by GPT-4, poses significant challenges that limit the usage of Artificial Intelligence (AI)-based systems powered by large Deep Learning (DL) models. These include rising computational demands for training and deployment and issues related to trustworthiness, bias, and interpretability. Such factors can make managing these models impractical for many organizations, while their "black-box” nature undermines key aspects, including transparency and accountability. In this paper, we question the prevailing assumption that increasing model parameters is always the optimal path forward, provided there is sufficient new data to learn additional patterns. In particular, we advocate for a Neurosymbolic research direction that combines the strengths of existing DL techniques (e.g., LLMs) with traditional symbolic methods–renowned for their reliability, speed, and determinism. To this end, we outline the core features and present preliminary results for our envisioned approach, aimed at establishing the first Neurosymbolic Program Comprehension (NsPC) framework to aid in identifying defective code components.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">velasco2025toward</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Toward Neurosymbolic Program Comprehension}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Velasco, Alejandro and Garryyeva, Aya and Palacio, David N and Mastropaolo, Antonio and Poshyvanyk, Denys}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{To Appear}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the 33rd IEEE/ACM International Conference on Program Comprehension (ICPC-ERA 2025) }</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Software engineering,  Intelligence, Program Comprehension, Neurosymbolic}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="afrin2025quantization" class="col-sm-8"> <div class="title">Is Quantization a Deal-breaker? Empirical Insights from Large Code Models</div> <div class="author"> Saima Afrin, Bowen Xu, and Antonio Mastropaolo </div> <div class="periodical"> <em>Proceedings of the 41st IEEE International Conference on Software Maintainance and Evolution (ICSME 2025) </em>, Jun 2025 </div> <div class="periodical"> To Appear </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2507.09665" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>The growing scale of large language models (LLMs) not only demands extensive computational resources but also raises environmental concerns due to their increasing carbon footprint. Model quantization emerges as an effective approach that can reduce the resource demands of LLMs by decreasing parameter precision without substantially affecting performance (e.g., 16 bit to 4 bit). While recent studies have established quantization as a promising approach for optimizing large code models (LCMs), a specialized subset of LLMs tailored for automated software engineering, their findings offer only limited insights into its practical implications. Specifically, current investigations focus only on the functional correctness of the code generated by quantized models, neglecting how quantization impacts critical aspects of code quality such as reliability, maintainability, and security. To bridge this gap, our study investigates the effects of quantization on the qualitative aspects of automatically generated code. We apply Activation-aware Weight Quantization (AWQ) to two widely used code models, CodeLlama and DeepSeekCoder, to generate Java and Python code. Using state-of-the-art static analysis tools, we evaluate software quality metrics and static features including cyclomatic complexity, cognitive complexity, and lines of code. Our findings reveal that quantization is a robust technique that not only preserves functional correctness, but also retains key qualitative code attributes sought after by developers, such as maintainability and structural simplicity.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">afrin2025quantization</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Is Quantization a Deal-breaker? Empirical Insights from Large Code Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Afrin, Saima and Xu, Bowen and Mastropaolo, Antonio}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the 41st IEEE International Conference on Software Maintainance and Evolution (ICSME 2025) }</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{To Appear}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Software engineering,  Quantization, Efficient Software Engineering, Code Quality}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="mastropaolo2025path" class="col-sm-8"> <div class="title">A Path Less Traveled: Reimagining Software Engineering Automation via a Neurosymbolic Paradigm</div> <div class="author"> Antonio Mastropaolo, and Denys Poshyvanyk </div> <div class="periodical"> <em>In Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering</em>, Apr 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://antoniomastropaolo.com/assets/pdf/AI_SDLC_Position.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>The emergence of Large Code Models (LCMs) has transformed software engineering (SE) automation, driving significant advancements in tasks such as code generation, source code documentation, code review, and bug fixing. However, these advancements come with trade-offs: achieving high performance often entails exponential computational costs, reduced interpretability, and an increasing dependence on data-intensive models with hundreds of billions of parameters. In this paper, we propose Neurosymbolic Software Engineering, in short NSE, as a promising paradigm combining neural learning with symbolic (rule-based) reasoning, while strategically introducing a controlled source of chaos to simulate the complex dynamics of real-world software systems. This hybrid methodology aims to enhance efficiency, reliability, and transparency in AI-driven software engineering, while introducing controlled randomness to adapt to evolving requirements, unpredictable system behaviors, and non-deterministic execution environments. By redefining the core principles of AI-driven software engineering automation, NSE lays the groundwork for solutions that are more adaptable, transparent, and closely aligned with the evolving demands of modern software development practices.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mastropaolo2025path</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Path Less Traveled: Reimagining Software Engineering Automation via a Neurosymbolic Paradigm}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mastropaolo, Antonio and Poshyvanyk, Denys}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1358--1362}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Software engineering,  Neurosymbolic, Position Paper, AI4SE, LLM4Code}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="10.1145/3709360" class="col-sm-8"> <div class="title">From Triumph to Uncertainty: The Journey of Software Engineering in the AI Era</div> <div class="author"> Antonio Mastropaolo, Camilo Escobar-Velásquez, and Mario Linares-Vásquez </div> <div class="periodical"> <em>ACM Trans. Softw. Eng. Methodol.</em>, Dec 2024 </div> <div class="periodical"> Just Accepted </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1145/3709360" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Over the last ten years, the realm of Artificial Intelligence (AI) has experienced an explosion of revolutionary breakthroughs, transforming what seemed like a far-off dream into a reality that is now deeply embedded in our everyday lives. AI’s widespread impact is revolutionizing virtually all aspects of human life, and software engineering (SE) is no exception. As we explore this changing landscape, we are faced with questions about what the future holds for SE and how AI will reshape the roles, duties, and methodologies within the field. The introduction of these groundbreaking technologies highlights the inevitable shift towards a new paradigm, suggesting a future where AI’s capabilities may redefine the boundaries of SE, potentially even more than human input.In this paper, we aim at outlining the key elements that, based on our expertise, are vital for the smooth integration of AI into SE, all while preserving the intrinsic human creativity that has been the driving force behind the field. First, we provide a brief description of SE and AI evolution. Afterward, we delve into the intricate interplay between AI-driven automation and human innovation, exploring how these two components can work together to advance SE practices to new methods and standards.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1145/3709360</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mastropaolo, Antonio and Escobar-Vel\'{a}squez, Camilo and Linares-V\'{a}squez, Mario}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{From Triumph to Uncertainty: The Journey of Software Engineering in the AI Era}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1049-331X}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Just Accepted}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Trans. Softw. Eng. Methodol.}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Software engineering, Artificial Intelligence, History, AI4SE, LLM4Code}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 The AURA Lab. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Icons from <a href="https://www.flaticon.com" target="_blank" rel="external nofollow noopener">Flaticon</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-ongoing-research",title:"Ongoing Research",description:"Research avenues we actively explore at the AURA Lab.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"Repositories",description:"Insights into the GitHub Repositories of AURA Lab Members. Discover the latest statistics and trends from the GitHub repositories maintained by the talented members of the AURA Lab. These repositories reflect our commitment to innovation and excellence in advancing sustainability, automation, and interpretability in software engineering",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-people",title:"People",description:"Members of the AURA Lab",section:"Navigation",handler:()=>{window.location.href="/people/"}},{id:"news-exciting-news-we-are-delighted-to-welcome-saima-to-the-aura-lab-starting-in-september-2024-saima-will-begin-her-research-journey-under-the-mentorship-of-prof-mastropaolo-we-are-eager-to-see-her-contributions-to-our-mission-of-advancing-sustainability-automation-and-interpretability-in-software-engineering-welcome-aboard-saima",title:"\ud83d\udd0a Exciting News! We are delighted to welcome Saima to the AURA Lab!...",description:"",section:"News"},{id:"news-exciting-announcement-we-are-thrilled-to-welcome-aya-and-joseph-to-the-aura-lab-starting-in-january-2025-they-will-embark-on-their-research-journeys-under-the-guidance-of-prof-mastropaolo-we-look-forward-to-their-contributions-to-advancing-sustainability-automation-and-interpretability-in-software-engineering-welcome-to-the-team-aya-and-joseph",title:"\ud83d\udd0a Exciting Announcement! We are thrilled to welcome Aya and Joseph to the...",description:"",section:"News"},{id:"news-exciting-news-our-work-from-triumph-to-uncertainty-the-journey-of-software-engineering-in-the-ai-era-has-been-officially-accepted-for-publication-in-tosem-acm-transactions-on-software-engineering-and-methodology",title:"\ud83d\udd25 Exciting news! Our work, \u201cFrom Triumph to Uncertainty: The Journey of Software...",description:"",section:"News"},{id:"news-exciting-update-our-paper-quot-optimizing-datasets-for-code-summarization-is-code-comment-coherence-enough-quot-has-been-accepted-to-the-icpc-2025-research-track-33rd-ieee-acm-international-conference-on-program-comprehension-shout-out-to-antonio-vitale-simone-scalabrino-max-di-penta-and-rocco-oliveto",title:"\ud83d\udd25 Exciting update! Our paper, &amp;quot;Optimizing Datasets for Code Summarization: Is Code-Comment Coherence...",description:"",section:"News"},{id:"news-exciting-news-our-paper-quot-toward-neuro-symbolic-program-comprehension-quot-has-been-accepted-for-publication-in-the-icpc-era-2025-part-of-the-33rd-ieee-acm-international-conference-on-program-comprehension-a-big-thank-you-to-my-incredible-collaborators-alejandro-velasco-aya-garryyeva-david-nader-palacio-and-denys-poshyvanyk",title:"\ud83c\udf1f Exciting news! Our paper, &amp;quot;Toward Neuro Symbolic Program Comprehension&amp;quot;, has been accepted...",description:"",section:"News"},{id:"news-exciting-news-for-saima-she-begins-her-phd-journey-with-a-publication-in-the-forge-2025-research-track-stay-tuned-for-the-pre-print-quot-resource-efficient-amp-amp-effective-code-summarization-quot",title:"\ud83c\udf89 Exciting news for Saima! She begins her PhD journey with a publication...",description:"",section:"News"},{id:"news-big-win-thrilled-to-annource-our-nsf-crii-shf-advancing-sustainable-software-engineering-practices-with-energy-efficient-large-language-models-for-code-2451058-pi-antonio-mastropaolo-june-1-2025-may-31-2027",title:"\ud83c\udfc6\ud83c\udfc6\ud83c\udfc6 Big Win!!! -- Thrilled to annource our NSF CRII SHF: Advancing Sustainable...",description:"",section:"News"},{id:"news-fantastic-news-the-position-paper-co-authored-with-denys-poshyvanyk-quot-a-path-less-traveled-reimagining-software-engineering-automation-via-a-neurosymbolic-paradigm-quot-has-been-accepted-for-publication-in-the-ai-sdlc-2025-the-workshop-is-co-located-with-the-acm-international-conference-on-the-foundations-of-software-engineering-fse-39-25-trondheim-norway",title:"\ud83c\udf1f \ud83d\udd25 Fantastic news! The position paper co-authored with Denys Poshyvanyk, &amp;quot;A Path...",description:"",section:"News"},{id:"news-exciting-update-our-paper-quot-is-quantization-a-deal-breaker-empirical-insights-from-large-code-models-quot-is-now-available-on-arxiv-shout-out-to-saima-afrin-bowen-xu-and-antonio-mastropaolo",title:"\ud83d\udd25 Exciting update! Our paper, &amp;quot;Is Quantization a Deal-breaker? Empirical Insights from Large...",description:"",section:"News"},{id:"projects-toward-efficient-software-engineering-automation",title:"Toward Efficient Software Engineering Automation",description:"Advancing Efficient and Sustainable Software Engineering Automation via, Quantization, Knowledge Distillation and PEFT",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-interpretable-neurosymbolic-software-engineering",title:"Interpretable Neurosymbolic Software Engineering",description:"Decoding the Synergy of Neural and Symbolic Systems in Software Engineering",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-extra-functional-requirements-of-large-code-models",title:"Extra Functional Requirements of Large Code Models",description:"Unveiling the Hidden Challenges of LCMs--Robustness, Trustworthiness and Security",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-towards-intelligent-and-scalable-software-documentation-practices-via-with-multi-agent-llms",title:"Towards Intelligent and Scalable Software Documentation Practices via with Multi-Agent LLMs",description:"Beyond Snippets \u2014 Challenges in Reasoning, Integration, and Coherence in Multi-Agent LLMs for Code Understanding",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>